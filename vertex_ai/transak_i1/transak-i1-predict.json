{
  "components": {
    "comp-batch-predict-op": {
      "executorLabel": "exec-batch-predict-op",
      "inputDefinitions": {
        "artifacts": {
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "vertex_model": {
            "artifactType": {
              "schemaTitle": "google.VertexModel",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "experiment_name": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "project": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-get-prediction-data-op": {
      "executorLabel": "exec-get-prediction-data-op",
      "inputDefinitions": {
        "parameters": {
          "month": {
            "parameterType": "NUMBER_INTEGER"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "query": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "prediction_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-get-production-model-op": {
      "executorLabel": "exec-get-production-model-op",
      "inputDefinitions": {
        "parameters": {
          "location": {
            "parameterType": "STRING"
          },
          "model_display_name": {
            "parameterType": "STRING"
          },
          "project": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "production_model": {
            "artifactType": {
              "schemaTitle": "google.VertexModel",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-run-monitoring-op": {
      "executorLabel": "exec-run-monitoring-op",
      "inputDefinitions": {
        "artifacts": {
          "prediction_table": {
            "artifactType": {
              "schemaTitle": "google.BQTable",
              "schemaVersion": "0.0.1"
            }
          },
          "vertex_model": {
            "artifactType": {
              "schemaTitle": "google.VertexModel",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "job_display_name": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "month": {
            "parameterType": "NUMBER_INTEGER"
          },
          "project": {
            "parameterType": "STRING"
          },
          "query_template": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "anomalies": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-save-predictions-op": {
      "executorLabel": "exec-save-predictions-op",
      "inputDefinitions": {
        "artifacts": {
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "bigquery_prediction_table_fqtn": {
            "parameterType": "STRING"
          },
          "month": {
            "parameterType": "NUMBER_INTEGER"
          },
          "pipeline_run_id": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "region": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "bigquery_prediction_table": {
            "artifactType": {
              "schemaTitle": "google.BQTable",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://af-finanzen-mlops/pipelines/transak-i1-predict",
  "deploymentSpec": {
    "executors": {
      "exec-batch-predict-op": {
        "container": {
          "command": [
            "python",
            "-m",
            "src.components.batch_predict.task",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--vertex-model-uri",
            "{{$.inputs.artifacts['vertex_model'].uri}}",
            "--test-data-uri",
            "{{$.inputs.artifacts['test_data'].uri}}",
            "--predictions-path",
            "{{$.outputs.artifacts['predictions'].path}}",
            "--experiment-name",
            "{{$.inputs.parameters['experiment_name']}}"
          ],
          "image": "europe-west6-docker.pkg.dev/af-finanzen/af-finanzen-mlops/transak-i1-train-predict:latest"
        }
      },
      "exec-get-prediction-data-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "get_prediction_data_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef get_prediction_data_op(\n    prediction_data: Output[Dataset],\n    project_id: str,\n    month: int,\n    query: str,\n):\n    \"\"\"\n    A component that executes a query for a given month and saves the result to a GCS path.\n    \"\"\"\n    import pandas as pd\n    from google.cloud import bigquery\n\n    final_query = query.format(month_placeholder=month)\n\n    bq_client = bigquery.Client(project=project_id)\n    print(f\"Running query: {final_query}\")\n    df = bq_client.query(final_query).to_dataframe()\n\n    print(f\"Saving prediction data to {prediction_data.path}\")\n    df.to_csv(prediction_data.path, index=False)\n\n"
          ],
          "image": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-17.py310"
        }
      },
      "exec-get-production-model-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "get_production_model_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform' 'google-cloud-pipeline-components' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\n\ndef get_production_model_op(\n    production_model: Output[VertexModel],\n    project: str,\n    location: str,\n    model_display_name: str,\n):\n    \"\"\"\n    A component that retrieves the Vertex AI Model artifact\n    currently aliased as 'production' in the Vertex AI Model Registry.\n    \"\"\"\n    from google.cloud import aiplatform\n\n    print(f\"Initializing AI Platform for project {project} in {location}...\")\n    aiplatform.init(project=project, location=location)\n\n    # Get the model resource (parent model) by display name\n    # models_with_display_name = aiplatform.Model.list(filter=f'display_name=\"{model_display_name}\"')\n    # model_versions = aiplatform.Model.listVersions(name=model_display_name)\n    # model_registry = aiplatform.models.ModelRegistry(model=model_id)\n        # Look for an existing model with the same display name to set as parent\n    print(f\"Searching for parent model with display name: {model_display_name}\")\n    model_resource_name = None\n    models = aiplatform.Model.list(\n        filter=f'display_name=\"{model_display_name}\"',\n        project=project,\n        location=location,\n    )\n    if models:\n        model_resource_name = models[0].resource_name\n        print(f\"Found model: {model_resource_name}\")\n    else:\n        print(\"No parent model found. A new model entry will be created.\")\n\n    production_model_version = aiplatform.Model(model_name=model_resource_name, version=\"production\")\n\n\n    if not production_model_version:\n        print(f\"No model found with display name: {model_display_name}\")\n        raise ValueError(\"No model found with the specified display name.\")\n    else:\n        print(f\"Found model with display_name: {production_model_version.display_name}\")\n\n    print(f\"Model: {production_model_version}\")\n\n    prod_model_resource_name = production_model_version.name\n\n    if prod_model_resource_name:\n        print(f\"Found production model: {prod_model_resource_name}\")\n        production_model.metadata[\"resourceName\"] = f\"{production_model_version.resource_name}@{production_model_version.version_id}\"\n        production_model.uri = production_model_version.uri\n        print(f\"Set production model resourceName to: {production_model.metadata['resourceName']}\")\n        print(f\"Set production model URI to: {production_model.uri}\")\n    else:\n        print(\"No model with 'production' alias found among versions.\")\n        raise ValueError(\"No production model found.\")\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-run-monitoring-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "run_monitoring_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform' 'google-cloud-pipeline-components' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\nfrom google_cloud_pipeline_components.types.artifact_types import BQTable\n\ndef run_monitoring_op(\n    project: str,\n    location: str,\n    vertex_model: Input[VertexModel],\n    prediction_table: Input[BQTable],\n    job_display_name: str,\n    month: int,\n    query_template: str,\n    anomalies: Output[Artifact],\n):\n    \"\"\"\n    Initiates a model monitoring job and outputs the URI to the anomalies file.\n\n    This component finds the ModelMonitor resource, triggers a monitoring job with a\n    predictable output path, and after completion, provides the GCS path to the\n    resulting anomalies.json file as an output artifact.\n    \"\"\"\n    import logging\n    from google.cloud import aiplatform\n    from vertexai.resources.preview import ml_monitoring\n\n    logging.getLogger().setLevel(logging.INFO)\n    aiplatform.init(project=project, location=location)\n\n    model_resource_name_with_version = vertex_model.metadata[\"resourceName\"]\n    logging.info(f\"Got model resource name: {model_resource_name_with_version}\")\n\n    model = aiplatform.Model(model_name=model_resource_name_with_version)\n    logging.info(f\"Model resource name: {model.resource_name}\")\n    logging.info(f\"Model version id: {model.version_id}\")\n\n    # The API does not support filtering by model resource name directly.\n    # We must list all monitors and filter them client-side.\n    monitors = ml_monitoring.ModelMonitor.list()\n\n    found_monitors = []\n    for monitor in monitors:\n        # The monitor's target model resource name (without version)\n        monitor_model_resource_name = monitor._gca_resource.model_monitoring_target.vertex_model.model\n        monitor_model_version_id = monitor._gca_resource.model_monitoring_target.vertex_model.model_version_id\n\n        if model.resource_name == monitor_model_resource_name and model.version_id == monitor_model_version_id:\n            found_monitors.append(monitor)\n            logging.info(f\"Found matching monitor: {monitor.resource_name}\")\n\n    if not found_monitors:\n        logging.warning(f\"No model monitor found for model '{model_resource_name_with_version}'. Skipping monitoring job.\")\n        return\n\n    if len(found_monitors) > 1:\n        raise ValueError(f\"Found {len(found_monitors)} monitors for model version {model_resource_name_with_version}. Expected only 1.\")\n\n    target_monitor = found_monitors[0]\n    logging.info(f\"Found model monitor: {target_monitor.resource_name}\")\n\n    # Construct the FQTN from the input artifact's metadata\n    bq_project = prediction_table.metadata[\"projectId\"]\n    bq_dataset = prediction_table.metadata[\"datasetId\"]\n    bq_table = prediction_table.metadata[\"tableId\"]\n    bigquery_prediction_table_fqtn = f\"{bq_project}.{bq_dataset}.{bq_table}\"\n\n    # Construct the BigQuery query for the specific month, performing the\n    # date logic directly in SQL for easier testing.\n    query = query_template.format(\n        month_placeholder=month,\n        table_placeholder=bigquery_prediction_table_fqtn\n    )\n\n    # query = f\"\"\"\n    # SELECT *\n    # FROM `{bigquery_prediction_table_fqtn}`\n    # WHERE \n    #     started_year = DIV({month}, 100) \n    #     AND started_month = MOD({month}, 100)\n    # \"\"\"\n    logging.info(f\"Using BigQuery query for monitoring: {query}\")\n\n    target_dataset = ml_monitoring.spec.MonitoringInput(\n        query=query,\n        data_format=\"bigquery\",\n    )\n\n    # Define a predictable output directory for the monitoring job.\n    # TODO: Make the base URI a component input parameter for more flexibility.\n    # monitoring_output_base_uri = \"gs://af-finanzen-mlops/monitoring\"\n    # output_uri = f\"{monitoring_output_base_uri}/{job_display_name}\"\n    # logging.info(f\"Monitoring job output will be saved to: {output_uri}\")\n    # output_spec = ml_monitoring.spec.OutputSpec(gcs_base_dir=output_uri)\n    output_spec = ml_monitoring.spec.OutputSpec(gcs_base_dir=anomalies.uri)\n\n    logging.info(f\"Starting monitoring job '{job_display_name}' for target data from BigQuery query.\")\n\n    # This is an async call\n    monitoring_job = target_monitor.run(\n        display_name=job_display_name,\n        target_dataset=target_dataset,\n        output_spec=output_spec,\n    )\n    monitoring_job.wait()\n\n    logging.info(f\"Successfully launched and completed monitoring job: {monitoring_job.resource_name}\")\n\n    # Extract the monitoring job ID from its resource name\n    # Example: projects/123/locations/europe-west6/modelMonitors/456/modelMonitoringJobs/789\n    monitoring_job_id = monitoring_job.resource_name.split('/')[-1]\n\n    # The anomalies file will be created at a predictable path based on the output_spec.\n    # The actual path includes the job ID and 'feature_drift' objective.\n    logging.info(f\"target_monitor name: {target_monitor.name}\")\n    logging.info(f\"monitoring_job_id: {monitoring_job_id}\")\n    anomalies_file_uri = f\"{anomalies.uri}/model_monitoring/{target_monitor.name}/tabular/jobs/{monitoring_job_id}/feature_drift/anomalies.json\"\n    logging.info(f\"Anomalies file URI: {anomalies_file_uri}\")\n\n    # The _dashboard_uri() attribute is not available. Constructing the URL manually.\n    dashboard_uri = (\n        f\"https://console.cloud.google.com/vertex-ai/locations/{location}/\"\n        f\"model-monitoring/{monitoring_job.resource_name}?project={project}\"\n    )\n    logging.info(f\"View job details in the console: {dashboard_uri}\")\n\n    anomalies.metadata[\"anomalies_file_uri\"] = anomalies_file_uri\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-save-predictions-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "save_predictions_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.2.2' 'google_cloud_pipeline_components==2.20.1' 'db-dtypes' 'pyarrow' 'google-cloud-bigquery' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import BQTable\n\ndef save_predictions_op(\n    predictions: Input[Artifact],\n    bigquery_prediction_table: Output[BQTable],\n    project_id: str,\n    region: str,\n    bigquery_prediction_table_fqtn: str,\n    pipeline_run_id: str,\n    month: int,\n):\n    \"\"\"\n    A component that saves batch predictions to a BigQuery table.\n    \"\"\"\n    import pandas as pd\n    from google.cloud import bigquery\n    import numpy as np\n    from pathlib import Path\n    import math\n\n    # --- Helper Functions ---\n    def softmax(x):\n        \"\"\"Compute softmax values for a single list of scores.\"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    def entropy(p):\n        \"\"\"Compute entropy for a single probability distribution.\"\"\"\n        return -sum([p_i * math.log(p_i) for p_i in p if p_i > 0])\n\n    # Find the prediction file and read it directly into Pandas\n    prediction_dir = Path(predictions.path)\n    jsonl_file = next(\n        (f for f in prediction_dir.iterdir() if \"prediction.results-\" in f.name),\n        None,\n    )\n    if not jsonl_file:\n        raise FileNotFoundError(f\"No prediction results file found in {predictions.path}\")\n\n    print(f\"Reading predictions from {jsonl_file} into DataFrame.\")\n    predictions_df = pd.read_json(jsonl_file, lines=True)\n\n    # The 'instance' column is a dict, and the 'prediction' column is a list of scores (logits).\n    # We want to flatten the instance and calculate our metrics.\n\n    # Get the original data from the 'instance' column\n    instance_df = pd.json_normalize(predictions_df['instance'])\n\n    # Get the logits\n    logits = list(predictions_df['prediction'])\n\n    # --- Calculate Metrics ---\n    predicted_class_ids = []\n    probabilities_list = []\n    confidence_msp_list = []\n    confidence_margin_list = []\n    confidence_entropy_list = []\n\n    for logit_list in logits:\n        # Probabilities\n        probabilities = softmax(np.array(logit_list))\n        probabilities_list.append(probabilities.tolist())\n\n        # Predicted class\n        predicted_class_ids.append(np.argmax(probabilities))\n\n        # MSP\n        confidence_msp_list.append(np.max(probabilities))\n\n        # Margin\n        sorted_probs = np.sort(probabilities)[::-1]\n        margin = sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else sorted_probs[0]\n        confidence_margin_list.append(margin)\n\n        # Entropy\n        confidence_entropy_list.append(entropy(probabilities))\n\n    # Create a new dataframe with the results\n    results_df = pd.concat([\n        instance_df,\n        pd.DataFrame({\n            'i1_pred_label_id': predicted_class_ids,\n            'logits': logits,\n            'probas': probabilities_list,\n            'confidence_msp': confidence_msp_list,\n            'confidence_margin': confidence_margin_list,\n            'confidence_entropy': confidence_entropy_list\n        })\n    ], axis=1)\n\n    # Add the pipeline run URL for traceability\n    if \"placeholder\" in pipeline_run_id.lower():\n        pipeline_run_url = \"local_run\"\n    else:\n        pipeline_run_url = f\"https://console.cloud.google.com/vertex-ai/pipelines/locations/{region}/runs/{pipeline_run_id}?project={project_id}\"\n    results_df[\"pipeline_run_url\"] = pipeline_run_url\n    results_df[\"month\"] = month\n\n    # Save the predictions to BigQuery\n    bq_client = bigquery.Client(project=project_id)\n    job_config = bigquery.LoadJobConfig(\n        write_disposition=\"WRITE_APPEND\",\n    )\n\n    print(f\"Saving predictions to {bigquery_prediction_table_fqtn}\")\n    job = bq_client.load_table_from_dataframe(\n        results_df, bigquery_prediction_table_fqtn, job_config=job_config\n    )\n    job.result()\n\n    # Update the output artifact\n    table = bq_client.get_table(bigquery_prediction_table_fqtn)\n    bigquery_prediction_table.metadata[\"tableId\"] = table.table_id\n    bigquery_prediction_table.metadata[\"datasetId\"] = table.dataset_id\n    bigquery_prediction_table.metadata[\"projectId\"] = table.project\n    bigquery_prediction_table.uri = f\"https://console.cloud.google.com/bigquery?project={table.project}&ws=!1m5!1m4!4m3!1s{table.project}!2s{table.dataset_id}!3s{table.table_id}\"\n\n"
          ],
          "image": "python:3.9"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "Artur's project Transak - Iteration 1 - An end-to-end pipeline to get predictions from the transaction classifier.",
    "name": "transak-i1-predict"
  },
  "root": {
    "dag": {
      "tasks": {
        "batch-predict-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-batch-predict-op"
          },
          "dependentTasks": [
            "get-prediction-data-op",
            "get-production-model-op"
          ],
          "inputs": {
            "artifacts": {
              "test_data": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "prediction_data",
                  "producerTask": "get-prediction-data-op"
                }
              },
              "vertex_model": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "production_model",
                  "producerTask": "get-production-model-op"
                }
              }
            },
            "parameters": {
              "experiment_name": {
                "runtimeValue": {
                  "constant": ""
                }
              },
              "location": {
                "componentInputParameter": "region"
              },
              "project": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "taskInfo": {
            "name": "Batch Predict: Production"
          }
        },
        "get-prediction-data-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-get-prediction-data-op"
          },
          "inputs": {
            "parameters": {
              "month": {
                "componentInputParameter": "month"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "query": {
                "runtimeValue": {
                  "constant": "\n        \n        SELECT\n              tid\n            , type\n            , EXTRACT(YEAR  FROM started) AS started_year\n            , EXTRACT(MONTH FROM started) AS started_month\n            , EXTRACT(DAY   FROM started) AS started_day\n            , MOD(EXTRACT(DAYOFWEEK FROM started) + 5, 7) AS started_weekday\n            , EXTRACT(YEAR  FROM first_started) AS first_started_year\n            , EXTRACT(MONTH FROM first_started) AS first_started_month\n            , EXTRACT(DAY   FROM first_started) AS first_started_day\n            , MOD(EXTRACT(DAYOFWEEK FROM first_started) + 5, 7) AS first_started_weekday\n            , LOWER(description) AS description\n            , amount\n            , currency\n    \n        \n        FROM `af-finanzen.banks.revolut_v`\n    \n        \n        WHERE \n            type NOT IN ('FEE', 'ATM') -- less then 10 examples of each type\n            AND ABS(amount) < 900 -- outliers\n            -- AND month < 202501 -- only for demonstration purposes to train bad model\n    \n        AND month = {month_placeholder}\n    "
                }
              }
            }
          },
          "taskInfo": {
            "name": "Get Prediction Data"
          }
        },
        "get-production-model-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-get-production-model-op"
          },
          "inputs": {
            "parameters": {
              "location": {
                "componentInputParameter": "region"
              },
              "model_display_name": {
                "componentInputParameter": "model_name"
              },
              "project": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "taskInfo": {
            "name": "Get Production Model"
          }
        },
        "run-monitoring-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-run-monitoring-op"
          },
          "dependentTasks": [
            "get-production-model-op",
            "save-predictions-op"
          ],
          "inputs": {
            "artifacts": {
              "prediction_table": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "bigquery_prediction_table",
                  "producerTask": "save-predictions-op"
                }
              },
              "vertex_model": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "production_model",
                  "producerTask": "get-production-model-op"
                }
              }
            },
            "parameters": {
              "job_display_name": {
                "runtimeValue": {
                  "constant": "transak-i1-monitor-{{$.inputs.parameters['pipelinechannel--month']}}"
                }
              },
              "location": {
                "componentInputParameter": "region"
              },
              "month": {
                "componentInputParameter": "month"
              },
              "pipelinechannel--month": {
                "componentInputParameter": "month"
              },
              "project": {
                "componentInputParameter": "project_id"
              },
              "query_template": {
                "runtimeValue": {
                  "constant": "\n        SELECT PREDICTIONS.*\n        , LABELS.name AS i1_pred_label\n        \n        FROM `{table_placeholder}` AS PREDICTIONS\n        LEFT JOIN `af-finanzen.transak.i1_labels` AS LABELS ON PREDICTIONS.i1_pred_label_id = LABELS.id\n    \n        \n        WHERE PREDICTIONS.month = {month_placeholder}\n    \n    "
                }
              }
            }
          },
          "taskInfo": {
            "name": "Run Model Monitoring"
          }
        },
        "save-predictions-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-save-predictions-op"
          },
          "dependentTasks": [
            "batch-predict-op"
          ],
          "inputs": {
            "artifacts": {
              "predictions": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "predictions",
                  "producerTask": "batch-predict-op"
                }
              }
            },
            "parameters": {
              "bigquery_prediction_table_fqtn": {
                "runtimeValue": {
                  "constant": "{{$.inputs.parameters['pipelinechannel--project_id']}}.transak.i1_predictions"
                }
              },
              "month": {
                "componentInputParameter": "month"
              },
              "pipeline_run_id": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_name}}"
                }
              },
              "pipelinechannel--project_id": {
                "componentInputParameter": "project_id"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "region": {
                "componentInputParameter": "region"
              }
            }
          },
          "taskInfo": {
            "name": "Save Predictions"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "model_name": {
          "defaultValue": "transak-i1-train-model",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "month": {
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "project_id": {
          "defaultValue": "af-finanzen",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "region": {
          "defaultValue": "europe-west6",
          "isOptional": true,
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.7.0"
}